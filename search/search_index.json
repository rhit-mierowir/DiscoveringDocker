{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"`/]+|\\.(?!\\d)|&[lg]t;|(?!\\b)(?=[A-Z][a-z])","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Discovering Docker","text":"<p>This is a repo noting my experiments with docker and any notes or information that I want to record. </p> <p>Hopefully this site will be fairly terse.</p>"},{"location":"docker_intro/","title":"Docker Introduction &amp; Concepts","text":"<p>This work was derrived from  the docker documentation, here.</p> <p>Any work for this will be in the <code>docker_intro</code> folder.</p> <p>The following sections are based on the lessons from the lessons.</p>"},{"location":"docker_intro/#get-docker-desktop","title":"Get Docker Desktop","text":"<p>The following code was used to create the welcome-to-docker image, which was then deployed as a server via docker desktop.</p> <pre><code>git clone https://github.com/docker/welcome-to-docker\ncd welcome-to-docker\ndocker build -t welcome-to-docker .\n</code></pre> <p>This all generates an image holding all of this configuration that can then be run as a container.</p>"},{"location":"docker_intro/#develop-with-containers","title":"Develop With Containers","text":"<p>To deploy the project, I ran the following code. I think this spins up multiple images.</p> <pre><code>git clone https://github.com/docker/getting-started-todo-app\ncd getting-started-todo-app\ndocker compose watch\n</code></pre> <p>This automatically installed Node, MySQL and other dependencies with almost zero effort.</p> <p>Changes to the running software were performed as quickly as the files were saved because docker automatically watches the involved files for changes and the files are shared in a containerized environment.</p>"},{"location":"docker_intro/#build-and-push-your-first-image","title":"Build and push your first image","text":"<p>I don't think that this is something we will be using, so I mostly just read through it. </p> <code>dockerfile/image</code> A text-based format to specify how to build an image. <p>Here are the commands that we would need to run:</p> <pre><code>git clone https://github.com/docker/getting-started-todo-app\ndocker build -t &lt;DOCKER_USERNAME&gt;/getting-started-todo-app .\ndocker image ls  #This is to check that the image exists locally.\ndocker push &lt;DOCKER_USERNAME&gt;/getting-started-todo-app\n</code></pre> <p>Docker-Hub is the go-to repository for docker images, both official and those distributed by other individuals.</p>"},{"location":"docker_intro/building-images/","title":"Building Images","text":""},{"location":"docker_intro/building-images/#understanding-image-layers","title":"Understanding Image Layers","text":"<p>Images are composed of layers, each of which is immutable once created. Each image contains a set of filesystem changes (additions, deletions, or modifications)</p> <p>An example of layers in an image for an app:</p> <pre><code>1. The first layer adds basic commands and a package manager, such as apt.\n2. The second layer installs a Python runtime and pip for dependency management.\n3. The third layer copies in an application\u2019s specific requirements.txt file.\n4. The fourth layer installs that application\u2019s specific dependencies.\n5. The fifth layer copies in the actual source code of the application.\n</code></pre> <p>This allows you to reuse layers, for instance, layers 1 &amp; 2 can be reused for another python program. It also lets you extend images created by others.</p>"},{"location":"docker_intro/building-images/#stacking-the-layers","title":"Stacking the layers","text":"<p>Layering is made possible by content-addressable storage and union filesystems. This allows you to reuse layers between images, but also to be able to run multiple containers from the same underlying image.</p> <p>Edited Quote from lesson:</p> <ol> <li>After each layer is downloaded, it is extracted into its own directory on the host filesystem.</li> <li>When you run a container from an image, a union filesystem is created where layers are stacked on top of each other, creating a new and unified view.<ol> <li>A new layer is also createdfor each running container, allowing each container to edit its filesystem without impacting others.</li> </ol> </li> <li>When the container starts, its root directory is set to the location of this unified directory, using chroot.</li> </ol> <code>Base Image</code> Although all images can be used to add layers onto, some layers have little utility on their own and are instead intended to serve as the foundation for other images. Such images are refered to as a Base Image, like the ones seen in this example."},{"location":"docker_intro/building-images/#example","title":"Example","text":"<p>First, we use an existing debian container to create a node container, which we use to create our app container.</p> <p>Here, <code>$</code> specifies that it is on be base terminal, <code>root@d8c5ca119fcd:/#</code> specifies that it is entered in the command line that appears in the container. </p> <pre><code>$ docker run --name=base-container -ti ubuntu\nroot@d8c5ca119fcd:/# apt update &amp;&amp; apt install -y nodejs\nroot@d8c5ca119fcd:/# node -e 'console.log(\"Hello world!\")'\n$ docker container commit -m \"Add node\" base-container node-base\n$ docker image history node-base\n$ docker run node-base node -e \"console.log('Hello again')\"\n$ docker rm -f base-container\n</code></pre> <p>Now we have created the node package from a base ubuntu container, and validated node was installed.</p> <pre><code>$ docker run --name=app-container -ti node-base\nroot@d8c5ca119fcd:/# echo 'console.log(\"Hello from an app\")' &gt; app.js\nroot@d8c5ca119fcd:/# node app.js\n$ docker container commit -c \"CMD node app.js\" -m \"Add app\" app-container sample-app\n$ docker image history sample-app\n$ docker run sample-app\n$ docker rm -f app-container\n</code></pre> <p>Now, we have created a container that will automatically run our trivial little program and print out a message when run.</p>"},{"location":"docker_intro/building-images/#summary","title":"Summary","text":"<p>Basically, you can imagine what happens to a docker container as sediment acrewing on the ocean floor. There is some base image, then people edit the container using the container-specific layer, and when that reaches a desired state they will commit it to a new image to lock in that state. </p> <p>Since this whole process was explained as file-system edits, I am not exactly sure how the commands passed into the container are preserved and re-run when the image is recreated, which was shown to happen when the app was finally constructed. </p>"},{"location":"docker_intro/building-images/#writing-a-dockerfile","title":"Writing a Dockerfile","text":"<p>A Dockerfile is a text file that allows you to transform an existing docker image into a new one by specifying the operations that must be done on it.  This involves commands like specifying the working directory, copying files into the container, and running commands in the container. Then, the CMD option specifies what should be run when you start the container.</p> <p>Each command corresponds to a new layer in your built image.</p> <p>For example:</p> <pre><code>FROM python:3.13\nWORKDIR /usr/local/app\n\n# Install the application dependencies\nCOPY requirements.txt ./\nRUN pip install --no-cache-dir -r requirements.txt\n\n# Copy in the source code\nCOPY src ./src\nEXPOSE 8080\n\n# Setup an app user so the container doesn't run as the root user\nRUN useradd app\nUSER app\n\nCMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8080\"]\n</code></pre> <p>Some of the most common instructions in a Dockerfile include: (Copied from course)</p> <ul> <li>FROM  - this specifies the base image that the build will extend. <li>WORKDIR  - this instruction specifies the \"working directory\" or the path in the image where files will be copied and commands will be executed. <li>COPY   - this instruction tells the builder to copy files from the host and put them into the container image. <li>RUN  - this instruction tells the builder to run the specified command. <li>ENV   - this instruction sets an environment variable that a running container will use. <li>EXPOSE  - this instruction sets configuration on the image that indicates a port the image would like to expose. <li>USER  - this instruction sets the default user for all subsequent instructions. <li>CMD [\"\", \"\"] - this instruction sets the default command a container using this image will run. <p>To read through all of the instructions or go into greater detail, check out the Dockerfile reference.</p> <p>If you have an existing project, you can run <code>docker init</code> and it will analyze the project and attempt to quickly create a dockerfile, a compose.yaml, and a .dockerignore file to containerize your project.</p> <p>Best practices make maintaining an image easier, use of the image quicker, and storage of the image and container more efficient.</p> <p>Docker File Best Practices</p>"},{"location":"docker_intro/building-images/#build-tag-and-publish-an-image","title":"Build, Tag, and Publish an Image","text":"<code>Building Images</code> The process of building an image based on a Dockerfile <code>Tagging Images</code> The process of giving an image a name, which also determines where the image can be distributed. <code>Publishing Images</code> The process to distribute or share the newly created image using a container registry."},{"location":"docker_intro/building-images/#building-images_1","title":"Building Images","text":"<p>To turn a Dockerfile into an immutable image, run:</p> <pre><code>docker build .\ndocker run &lt;name of image&gt;\n</code></pre> <p>The dot specifies the folder looked in for the Dockerfile and the files referanced in the Dockerfile. The images created will be be named as a sha hash. For a more interpretable name, use a tag.</p>"},{"location":"docker_intro/building-images/#adding-tags","title":"Adding Tags","text":"<p>The general format for a tag is as follows:</p> <p><code>[HOST[:PORT_NUMBER]/]PATH[:TAG]</code></p> <ul> <li><code>HOST</code>: The optional registry hostname where the image is located. If no host is specified, Docker's public registry at docker.io is used by default.</li> <li><code>PORT_NUMBER</code>: The registry port number if a hostname is provided</li> <li><code>PATH</code>: The path of the image, consisting of slash-separated components. For Docker Hub, the format follows [NAMESPACE/]REPOSITORY, where namespace is either a user's or organization's name. If no namespace is specified, library is used, which is the namespace for Docker Official Images.</li> <li><code>TAG</code>: A custom, human-readable identifier that's typically used to identify different versions or variants of an image. If no tag is specified, latest is used by default.</li> </ul> <p>Use <code>-t</code> or <code>--tag</code> flag to add a tag to a container you are building, or if you have the image already, use <code>docker image tag</code></p> <pre><code>docker build -t my-username/my-image .\ndocker image tag my-username/my-image another-username/another-image:v1\n</code></pre> <p>The same tag can refer to different images at different points in time, allowing for you to update the image without everyone having to change their Dockerfiles to match.</p>"},{"location":"docker_intro/building-images/#publishing-images","title":"Publishing Images","text":"<p>To push an image that you have already built and tagged to a registry, use <code>docker push</code>:</p> <pre><code>docker push my-username/my-image\n</code></pre>"},{"location":"docker_intro/building-images/#using-the-build-cache","title":"Using the Build Cache","text":"<p>To avoid repeating steps that have already occoured when building a previous version of an image from a Dockerfile, the build cache stores the results of these steps so you can skip them for a faster complilation, however if any element that impacts one step changes, it invalidates that step and all those after it, which will be rerun. </p> <p>Each layer on an image will be reused by the Build Cache if no changes are detected on anything impacting that layer(command) or an earlier one.</p>"},{"location":"docker_intro/building-images/#multi-stage-builds","title":"Multi-Stage Builds","text":"<p>While it works perfectly well to build everything in one container, that often leaves unnecessary bloat, as the complilation software or setup software is rarely used in the final application. This lets the docker container take up less space on the disk and also exposes it to fewer cybersecurity vulnerabilities because there is less to attack. </p> <p>This can be done by having two <code>FROM</code> commands that produce two different docker containers, only one of which will be the final build, which is by default the second one in the list, but there are commands to specify this explicitly. For this, you can start with a compilation-based image to compile the code, then copy this code from the first image into the second without having to include any information unnecessary to run the application. An example of this structure is shown below.</p> <pre><code># Stage 1: Build Environment\nFROM builder-image AS build-stage \n# Install build tools (e.g., Maven, Gradle)\n# Copy source code\n# Build commands (e.g., compile, package)\n\n# Stage 2: Runtime environment\nFROM runtime-image AS final-stage  \n#  Copy application artifacts from the build stage (e.g., JAR file)\nCOPY --from=build-stage /path/in/build/stage /path/to/place/in/final/stage\n# Define runtime configuration (e.g., CMD, ENTRYPOINT) \n</code></pre>"},{"location":"docker_intro/concepts/","title":"Concepts","text":"<p>This is where we are describing what I find in the <code>Docker Concept</code> section.</p> <p>This will also go in roughly the order Basics -&gt; Building -&gt; Running.</p>"},{"location":"docker_intro/concepts/#the-basics","title":"The Basics","text":""},{"location":"docker_intro/concepts/#what-is-a-container","title":"What is a container?","text":"<code>Container</code> An isolated process for each component of your app, with each component running in its own completely isolated environment. <p>Container Properties:</p> <ul> <li>Self-contained. Each container has no reliance on pre-installed dependencies on the host computer.</li> <li>Isolated. Increased security because each container has minimal influence on other containers.</li> <li>Independent. Each is independantly managed, so removing one doesn't impact others.</li> <li>Portable. - Each container works the same way everywhere.</li> </ul> <p>Continers function as Virtual Machines, but they remove most of the unnecessary overhead, only running a particular process.</p> <p>How to check for running containers:</p> <pre><code>docker ps\n</code></pre> <p>Run an example:</p> <pre><code>docker run -d -p 8080:80 docker/welcome-to-docker\n</code></pre> <p>This exposes port 80 of the container to port 8080 of the host device. Access via http://localhost:8080/.</p>"},{"location":"docker_intro/concepts/#what-is-an-image","title":"What Is an Image?","text":"<code>Docker Image</code> Everything required to run a container. a standardized package that includes all of the files, binaries, libraries, and configurations to run a container. <p>Images are immutable, so can't be changed, but you an add layers on top of docker images to add or remove files and functionality. This allows you to - for example - run python files on a pre-configured docker container image.</p> <p>This image can be distributed to give everything needed to run the container on things like dockerHub.</p> <p>If there are issues in an image (security vulnerability) you should replace them with a new and improved image, not change the image. </p> <p>Dockerfiles are somehow involved in this process.</p> <p>There are two important principles of images:</p> <ol> <li>Images are immutable. Once an image is created, it can't be modified. You can only make a new image or add changes on top of it.</li> <li>Container images are composed of layers. Each layer represents a set of file system changes that add, remove, or modify files.</li> </ol> <p>Docker Hub is the default place to source images, but there are other sources that you can get them from.</p> <pre><code>docker search docker/welcome-to-docker\ndocker pull docker/welcome-to-docker\ndocker image ls\ndocker image history docker/welcome-to-docker\n</code></pre> <p>This shows us looking on dockerhub for the container, pulling it to our local computer, validating that we have it, then checking the layers on that image.</p>"},{"location":"docker_intro/concepts/#what-is-a-registry","title":"What is a Registry?","text":"<p>Registrys are repositories where images are stored, allowing you to share across teams. These are things like DockerHub. There is an API protocall (OCI spec) that is open source and available to be implemented by anyone. Without registrys Docker would work perfectly well, but each user would be forced to operate in isolation and wouldn't be able to share their images.</p> <p>A <code>Registry</code> is slightly different from a <code>Repository</code>. </p> <code>Registry</code> a centralized location that stores and manages container images <code>Repository</code> a collection of related container images within a registry.  Think of it as a folder where you organize your images based on projects. <p>Each repository contains one or more container images, and a registry contains one or more repositories.</p>"},{"location":"docker_intro/concepts/#what-is-docker-compose","title":"What is Docker Compose?","text":"<p>While one container can run as many processes in it as you would like, this isn't good practice. Generally, you should set up or pull containers that do only one thing, and do it well.</p> <p>To do more advanced tasks that require the use of multiple containers, you can use <code>Docker Compose</code> to coordinate the multiple <code>docker run</code> commands and start multiple containers, each with specific configuration. Otherwise, the task of running multiple containers is very error prone and cleanup is complicated. Docker Compose allows you to declare this structure in a yaml file that can simply be run once defined as many times as desired with a single command. To deploy containers, or adjust containers according to an edit, run <code>docker compose up</code>.</p> <pre><code>docker compose up -d --build\ndocker compose down\ndocker compose down --volumes\n</code></pre> <p>The top <code>down</code> command just tares down the containers, but leaves the volumes created as persistant data, whereas the ones below it also tares down the volumes and deletes the related data.</p>"},{"location":"docker_intro/docker-desktop/","title":"Docker Desktop","text":"<p>The guide is pushing the use of docker desktop to visualize what is happening. I am finding it mildly challenging to install.</p> <p>I was following the guidance in the main file, but there seemed to be an instruction that it missed when constructing the container. It was encapsulated in a popup on the right side of docker desktop titled How do I run a container?. It suggested running the following commands to run the container. I fear that these instructions won't give me much insight into the actual cmd interface.</p> <pre><code>git clone https://github.com/docker/welcome-to-docker\ncd welcome-to-docker\ndocker build -t welcome-to-docker .\n</code></pre> <p>This all generates an image holding all of this configuration that can then be run as a container.</p>"},{"location":"docker_intro/running-containers/","title":"Running Containers","text":""},{"location":"docker_intro/running-containers/#publishing-and-exposing-ports","title":"Publishing and Exposing Ports","text":"<p>Docker containers generally run isolated from the rest of the system, but sometimes you want to interact with what they are publishing on their ports or expose them from ports on the host computer. This is where publishing ports comes in. </p> <p>In Dockerfiles, there is an <code>EXPOSE</code> command. This doesn't automatically publish the ports when the container is run, but simply makes the promice that this image is actually using this port and thus publishing it would be useful. </p> <p>To publish ports, use the <code>-p</code> command to specify the ports that you want to publish as shown below. The first commands specifies the exact port on the host computer to connect to, while the second lets the computer match it to an ephemeral port, and you can see the port chosen by running <code>docker ps</code>.</p> <pre><code>docker run -d -p HOST_PORT:CONTAINER_PORT nginx\ndocker run -p CONTAINER_PORT nginx\n</code></pre> <p>If you use the <code>-P</code> flag, then all ports exposed with <code>EXPOSE</code> commands in the dockerfile will automatically be mapped to ephemeral ports.</p> <pre><code>docker run -P nginx\n</code></pre>"},{"location":"docker_intro/running-containers/#overriding-defaults","title":"Overriding Defaults","text":"<p>Adding arguments to the <code>docker run</code> command allow it to automaticall override defaults of the container and customize it to your particular usecase</p>"},{"location":"docker_intro/running-containers/#fixing-port-conflicts","title":"Fixing port conflicts","text":"<p>If two services want to use the same port, simply map them to different ports when you are exposing them.</p> <pre><code>docker run -d -p HOST_PORT:CONTAINER_PORT postgres\n</code></pre> <p>You can also create an internal network to allow you to connect to more ports or to enable docker containers to connect to eachother more easily. </p> <pre><code>docker network create NETWORK_NAME\ndocker run -d -p HOST_PORT:CONTAINER_PORT --network NETWORK_NAME postgres\n</code></pre>"},{"location":"docker_intro/running-containers/#changing-environment-variables","title":"Changing Environment Variables","text":"<p>Set environment variables explicitly in the <code>docker run</code> command, or pass them in through a <code>.env</code> file.</p> <pre><code>docker run -e foo=bar postgres env\ndocker run --env-file .env postgres env\n</code></pre>"},{"location":"docker_intro/running-containers/#set-resource-limits","title":"Set Resource Limits","text":"<p>To restrict the amount of memory or cpus a container is allowed to use, set <code>--memory</code> or <code>--cpus</code> flags.</p> <pre><code>docker run -e POSTGRES_PASSWORD=secret --memory=\"512m\" --cpus=\"0.5\" postgres\n</code></pre>"},{"location":"docker_intro/running-containers/#persisting-container-data-docker-volumes","title":"Persisting Container Data (Docker Volumes)","text":"<p>Since Containers are ephemeral and we often want data to be persisted between the running of containers, we can use <code>volumes</code> to store data in a permanent way independant of the container that is using it and independantly of the host file system. It is likely easier to manage volumes from the desktop, including ways to be able to look through the contents of volumes, but I shall remain on the commandline. </p> <p>Creating a Volume:</p> <pre><code>docker volume create log-data\n</code></pre> <p>Managing volumes:</p> <ul> <li><code>docker volume ls</code> - list all volumes</li> <li><code>docker volume rm</code>  - remove a volume (only works when the volume is not attached to any containers) <li><code>docker volume prune</code> - remove all unused (unattached) volumes</li> <p>Creating A Container Using a Volume:</p> <pre><code>docker run -d -p 80:80 -v log-data:/logs docker/welcome-to-docker\n</code></pre>"},{"location":"docker_intro/running-containers/#sharing-local-files-with-containers","title":"Sharing Local Files With Containers","text":"<p>This can be done via <code>bind mounts</code>, and these are different from volumes because they link files in the container to files on your host whereas volumes mount to an empty directory managed by Docker. Technically, volumes are specific type of bind mount. If you want to persist information, use a volume. If you want to access a file on the host system, use a bind mount.</p> <p>The <code>-v</code> flag is simpler and more convenient for basic volume or bind mount operations. If the host location doesn\u2019t exist when using <code>-v</code> or <code>--volume</code>, a directory will be automatically created.</p> <p>The <code>--mount</code> flag offers more advanced features and granular control, making it suitable for complex mount scenarios or production deployments. If you use <code>--mount</code> to bind-mount a file or directory that doesn't yet exist on the Docker host, the docker run command doesn't automatically create it for you but generates an error.</p> <pre><code>docker run -v /HOST/PATH:/CONTAINER/PATH -it nginx\ndocker run --mount type=bind,source=/HOST/PATH,target=/CONTAINER/PATH,readonly nginx\n</code></pre> <p>You can also manage File permissions to determine if the mounted files can be edited, which are changes that would be reflected in the host. To grant read/write access, you can use the <code>:ro</code> flag (read-only) or <code>:rw</code> (read-write) with the <code>-v</code> or <code>--mount</code> flag during container creation</p> <pre><code>docker run -v HOST-DIRECTORY:/CONTAINER-DIRECTORY:rw nginx\n</code></pre>"},{"location":"docker_intro/running-containers/#multi-container-applications","title":"Multi-Container Applications","text":"<p>If your application requires running multiple docker containers, you can build and run them individually, which is error-prone and slow, or you can simply use docker compose. This can be done simply with the following command, where the <code>--build</code> argument simply specifies to build the docker containers that you have constructed locally using DockerFiles. Not only does this simplify building and running all of the containers, but it also makes the application easier to manage, as all the associated containers are grouped, and their relationships are directly encoded in the <code>compose.yml</code> file.</p> <pre><code>docker compose up -d --build\n</code></pre>"}]}